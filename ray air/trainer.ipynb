{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06f4add1",
   "metadata": {},
   "source": [
    "## Ray AIR : Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185d168f",
   "metadata": {},
   "source": [
    "### Deep Learning Trainers\n",
    "\n",
    "Ray Train offer 3 main deep learning trainers: `TorchTrainer`, `TensorflowTrainer`, and `HorovodTrainer`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deda6d7a",
   "metadata": {},
   "source": [
    "#### PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee9e50ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import ray\n",
    "from ray import train\n",
    "from ray.air import session, Checkpoint\n",
    "from ray.train.torch import TorchTrainer\n",
    "from ray.air.config import ScalingConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9af8410f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'input_size': 1,\n",
    "    'layer_size': 15,\n",
    "    'output_size': 1,\n",
    "    'num_epochs': 20\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e9bbb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(config['input_size'], config['layer_size'])\n",
    "        self.fc2 = nn.Linear(config['layer_size'], config['output_size'])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "def train_loop_per_worker():\n",
    "    dataset_shard = session.get_dataset_shard('train')\n",
    "    model = Network()\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
    "    \n",
    "    model = train.torch.prepare_model(model)\n",
    "    for epoch in range(config['num_epochs']):\n",
    "        for batches in dataset_shard.iter_torch_batches(batch_size=32, dtypes=torch.float):\n",
    "            inputs, labels = torch.unsqueeze(batches['x'], 1), batches['y']\n",
    "            output = model(inputs)\n",
    "            loss = loss_fn(output, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(f'epoch: {epoch}, loss; {loss.item()}')\n",
    "        \n",
    "        session.report(\n",
    "            {},\n",
    "            checkpoint=Checkpoint.from_dict(dict(epoch=epoch, model=model.state_dict())),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8aae6e4",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-18 17:36:43 (running for 00:00:10.45)<br>Memory usage on this node: 9.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/yjkim/ray_results/TorchTrainer_2022-10-18_17-36-33<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  _timestamp</th><th style=\"text-align: right;\">  _time_this_iter_s</th><th style=\"text-align: right;\">  _training_iteration</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_f8638_00000</td><td>TERMINATED</td><td>127.0.0.1:11891</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         6.76346</td><td style=\"text-align: right;\">  1666082202</td><td style=\"text-align: right;\">           0.210664</td><td style=\"text-align: right;\">                   20</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m 2022-10-18 17:36:38,761\tINFO config.py:71 -- Setting up process group for: env:// [rank=0, world_size=3]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m 2022-10-18 17:36:38,906\tINFO train_loop_utils.py:300 -- Moving model to device: cpu\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m 2022-10-18 17:36:38,906\tINFO train_loop_utils.py:347 -- Wrapping provided model in DDP.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 0, loss; 45093.17578125\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 0, loss; 25554690048.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 0, loss; 54554.171875\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 0, loss; 22445670400.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 0, loss; 51718.91015625\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 0, loss; 25215422464.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 0, loss; 882762.5\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 0, loss; 980065.0625\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 0, loss; 1078653.0\n",
      "Result for TorchTrainer_f8638_00000:\n",
      "  _time_this_iter_s: 0.41376280784606934\n",
      "  _timestamp: 1666082199\n",
      "  _training_iteration: 1\n",
      "  date: 2022-10-18_17-36-39\n",
      "  done: false\n",
      "  experiment_id: 6af9a4d21a064a91b0cf281e476862ec\n",
      "  hostname: YONGJINs-MacBook-Pro.local\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  pid: 11891\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 3.3061208724975586\n",
      "  time_this_iter_s: 3.3061208724975586\n",
      "  time_total_s: 3.3061208724975586\n",
      "  timestamp: 1666082199\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: f8638_00000\n",
      "  warmup_time: 0.017971038818359375\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/ray/air/_internal/torch_utils.py:122: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:178.)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m   return torch.as_tensor(ndarray, dtype=dtype, device=device)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/ray/air/_internal/torch_utils.py:122: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:178.)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m   return torch.as_tensor(ndarray, dtype=dtype, device=device)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/ray/air/_internal/torch_utils.py:122: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:178.)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m   return torch.as_tensor(ndarray, dtype=dtype, device=device)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m   return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 1, loss; 1000791.625\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 1, loss; 1060378.75\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 1, loss; 849056.875\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 1, loss; 1059024.5\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 1, loss; 1037115.4375\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 1, loss; 944651.8125\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 1, loss; 1041520.625\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 1, loss; 1048851.25\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 1, loss; 1041540.25\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 2, loss; 930459.875\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 2, loss; 961454.4375\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 2, loss; 965653.375\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 2, loss; 60474158743552.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 2, loss; 1677171200.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 2, loss; 53122340749312.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 2, loss; 1684072576.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 2, loss; 59664263806976.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 2, loss; 1687935744.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 3, loss; 1321073958191104.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 3, loss; 56705396736.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 3, loss; 55978647552.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 3, loss; 1602484241956864.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 3, loss; 56701095936.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 3, loss; 56004665344.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 3, loss; 1517729504821248.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 3, loss; 56702517248.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 3, loss; 56026914816.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 4, loss; 55339618304.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 4, loss; 55354052608.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 4, loss; 55349379072.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 4, loss; 54694584320.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 4, loss; 53992767488.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 4, loss; 54689898496.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 4, loss; 54018318336.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 4, loss; 54691483648.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 4, loss; 54040174592.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 5, loss; 53376942080.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 5, loss; 52755083264.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 5, loss; 53391118336.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 5, loss; 52750483456.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 5, loss; 53386526720.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 5, loss; 52752039936.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 5, loss; 52077314048.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 5, loss; 52102406144.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 5, loss; 52123869184.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 6, loss; 51483869184.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 6, loss; 51497791488.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 6, loss; 51493285888.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 6, loss; 50884362240.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 6, loss; 50229805056.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 6, loss; 50879844352.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 6, loss; 50254454784.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 6, loss; 50881376256.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 6, loss; 50275532800.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 7, loss; 49657942016.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 7, loss; 49079980032.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 7, loss; 48447840256.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 7, loss; 49671610368.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 7, loss; 49075552256.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 7, loss; 48472047616.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 7, loss; 49667186688.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 7, loss; 49077047296.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 7, loss; 48492748800.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 8, loss; 47896768512.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 8, loss; 47339593728.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 8, loss; 46729076736.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 8, loss; 47910199296.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 8, loss; 47335243776.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 8, loss; 46752849920.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 8, loss; 47905845248.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 8, loss; 47336710144.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 8, loss; 46773182464.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 9, loss; 46198059008.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 9, loss; 45660917760.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 9, loss; 45071269888.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 9, loss; 46211244032.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 9, loss; 45656641536.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 9, loss; 45094617088.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 9, loss; 46206976000.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 9, loss; 45658087424.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 9, loss; 45114580992.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 10, loss; 44559585280.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 10, loss; 44041773056.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 10, loss; 43472269312.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 10, loss; 44572532736.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 10, loss; 44037570560.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 10, loss; 43495194624.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 10, loss; 44568346624.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 10, loss; 44038987776.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 10, loss; 43514806272.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 11, loss; 42979221504.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 11, loss; 42991943680.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 11, loss; 42987819008.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 11, loss; 42480041984.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 11, loss; 41929990144.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 11, loss; 42475921408.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 11, loss; 41952509952.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 11, loss; 42477314048.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 11, loss; 41971769344.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 12, loss; 41454907392.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 12, loss; 40973697024.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 12, loss; 40442417152.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 12, loss; 41467404288.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 12, loss; 40969646080.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 12, loss; 40464531456.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 12, loss; 41463361536.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 12, loss; 40971018240.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 12, loss; 40483446784.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 13, loss; 39984660480.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 13, loss; 39996923904.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 13, loss; 39992954880.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 13, loss; 39520772096.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 13, loss; 39007608832.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 13, loss; 39516786688.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 13, loss; 39029325824.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 13, loss; 39518138368.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 13, loss; 39047905280.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 14, loss; 38566543360.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 14, loss; 38119370752.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 14, loss; 37623693312.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 14, loss; 38578601984.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 14, loss; 38115459072.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 14, loss; 37645025280.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 14, loss; 38574698496.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 14, loss; 38116786176.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 14, loss; 37663268864.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 15, loss; 37198725120.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 15, loss; 37210566656.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 15, loss; 37206736896.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 15, loss; 36767662080.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 15, loss; 36288868352.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 15, loss; 36763824128.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 15, loss; 36309819392.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 15, loss; 36765122560.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 15, loss; 36327735296.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 16, loss; 35879428096.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 16, loss; 35463892992.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 16, loss; 35891048448.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 16, loss; 35460124672.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 16, loss; 35887288320.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 16, loss; 35461398528.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 16, loss; 35039563776.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 16, loss; 35001393152.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 16, loss; 35021971456.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 17, loss; 34606915584.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 17, loss; 34206359552.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 17, loss; 33759584256.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 17, loss; 34618331136.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 17, loss; 34202658816.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 17, loss; 33779789824.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 17, loss; 34614636544.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 17, loss; 34203908096.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 17, loss; 33797072896.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 18, loss; 33379528704.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 18, loss; 33390739456.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 18, loss; 33387112448.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 18, loss; 32993415168.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 18, loss; 32561823744.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 18, loss; 32989777920.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 18, loss; 32581670912.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 18, loss; 32991008768.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 18, loss; 32598642688.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 19, loss; 32195670016.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 19, loss; 32206682112.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 19, loss; 32203120640.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 19, loss; 31823486976.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11897)\u001b[0m epoch: 19, loss; 31406559232.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 19, loss; 31819919360.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11896)\u001b[0m epoch: 19, loss; 31426048000.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 19, loss; 31821129728.0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=11898)\u001b[0m epoch: 19, loss; 31442714624.0\n",
      "Result for TorchTrainer_f8638_00000:\n",
      "  _time_this_iter_s: 0.2106640338897705\n",
      "  _timestamp: 1666082202\n",
      "  _training_iteration: 20\n",
      "  date: 2022-10-18_17-36-42\n",
      "  done: true\n",
      "  experiment_id: 6af9a4d21a064a91b0cf281e476862ec\n",
      "  experiment_tag: '0'\n",
      "  hostname: YONGJINs-MacBook-Pro.local\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 127.0.0.1\n",
      "  pid: 11891\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 6.7634618282318115\n",
      "  time_this_iter_s: 0.20285701751708984\n",
      "  time_total_s: 6.7634618282318115\n",
      "  timestamp: 1666082202\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 20\n",
      "  trial_id: f8638_00000\n",
      "  warmup_time: 0.017971038818359375\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-18 17:36:43,696\tINFO tune.py:758 -- Total run time: 10.58 seconds (10.44 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ray.data.from_items([{\"x\": x, \"y\": 2 * x + 1} for x in range(200)])\n",
    "scaling_config = ScalingConfig(num_workers=3, use_gpu=False)\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_loop_per_worker,\n",
    "    scaling_config=scaling_config,\n",
    "    datasets={'train': train_dataset},\n",
    ")\n",
    "\n",
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f34d77b",
   "metadata": {},
   "source": [
    "#### Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d94844f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from ray.air import session\n",
    "from ray.air.callbacks.keras import Callback\n",
    "from ray.train.tensorflow import prepare_dataset_shard\n",
    "from ray.train.tensorflow import TensorflowTrainer\n",
    "from ray.air.config import ScalingConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "87089c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model() -> tf.keras.Model:\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.InputLayer(input_shape=()),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(10),\n",
    "            tf.keras.layers.Dense(1),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_func(cofnig: dict):\n",
    "    batch_size = config.get(\"batch_size\", 64)\n",
    "    epochs = config.get('epochs', 10)\n",
    "    \n",
    "    strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "    with strategy.scope():\n",
    "        multi_worker_model = build_model()\n",
    "        multi_worker_model.compile(\n",
    "            optimizer=tf.keras.optimizers.SGD(learning_rate=config.get('lr', 1e-3)),\n",
    "            loss=tf.keras.losses.mean_squared_error,\n",
    "            metrics=[tf.keras.metrics.mean_squared_error],\n",
    "        )\n",
    "        \n",
    "    dataset = session.get_dataset_shard('train')\n",
    "    \n",
    "    def to_tf_dataset(dataset, batch_size):\n",
    "        def to_tensor_iterator():\n",
    "            for batch in dataset.iter_tf_batches(\n",
    "                batch_size=batch_size, dtypes=tf.float32\n",
    "            ):\n",
    "                yield batch['x'], batch['y']\n",
    "                \n",
    "        output_signature = (\n",
    "            tf.TensorSpec(shape=(None), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(None), dtype=tf.float32),\n",
    "        )\n",
    "        tf_dataset = tf.data.Dataset.from_generator(\n",
    "            to_tensor_iterator, output_signature=output_signature\n",
    "        )\n",
    "        \n",
    "        return prepare_dataset_shard(tf_dataset)\n",
    "    \n",
    "    result = []\n",
    "    for _ in range(epochs):\n",
    "        tf_dataset = to_tf_dataset(dataset=dataset, batch_size=batch_size)\n",
    "        history = multi_worker_model.fit(tf_dataset, callbacks=[Callback()])\n",
    "        result.append(history.history)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d701926f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-18 18:19:09 (running for 00:00:16.63)<br>Memory usage on this node: 9.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/4.24 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/yjkim/ray_results/TensorflowTrainer_2022-10-18_18-18-52<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  loss</th><th style=\"text-align: right;\">  mean_squared_error</th><th style=\"text-align: right;\">  _timestamp</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TensorflowTrainer_e22d0_00000</td><td>TERMINATED</td><td>127.0.0.1:13685</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         10.5579</td><td style=\"text-align: right;\">   nan</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">  1666084749</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13695)\u001b[0m 2022-10-18 18:19:04.285909: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13695)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13695)\u001b[0m 2022-10-18 18:19:04.289391: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> 127.0.0.1:53021, 1 -> 127.0.0.1:53022}\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13695)\u001b[0m 2022-10-18 18:19:04.289512: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> 127.0.0.1:53021, 1 -> 127.0.0.1:53022}\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13695)\u001b[0m 2022-10-18 18:19:04.291473: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://127.0.0.1:53022\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m 2022-10-18 18:19:04.285889: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m 2022-10-18 18:19:04.289469: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> 127.0.0.1:53021, 1 -> 127.0.0.1:53022}\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m 2022-10-18 18:19:04.289552: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> 127.0.0.1:53021, 1 -> 127.0.0.1:53022}\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m 2022-10-18 18:19:04.290225: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:438] Started server with target: grpc://127.0.0.1:53021\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13695)\u001b[0m 2022-10-18 18:19:04.580193: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m 2022-10-18 18:19:04.580195: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13695)\u001b[0m 2022-10-18 18:19:06.744702: W tensorflow/core/common_runtime/forward_type_inference.cc:231] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected compatible input types, but input 1:\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13695)\u001b[0m type_id: TFT_OPTIONAL\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13695)\u001b[0m args {\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13695)\u001b[0m   type_id: TFT_PRODUCT\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13695)\u001b[0m   args {\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13695)\u001b[0m     type_id: TFT_TENSOR\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13695)\u001b[0m     args {\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13695)\u001b[0m       type_id: TFT_BOOL\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13695)\u001b[0m     }\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13695)\u001b[0m   }\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13695)\u001b[0m }\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13695)\u001b[0m  is neither a subtype nor a supertype of the combined inputs preceding it:\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13695)\u001b[0m type_id: TFT_OPTIONAL\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13695)\u001b[0m args {\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13695)\u001b[0m   type_id: TFT_PRODUCT\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13695)\u001b[0m   args {\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13695)\u001b[0m     type_id: TFT_TENSOR\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13695)\u001b[0m     args {\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13695)\u001b[0m       type_id: TFT_LEGACY_VARIANT\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13695)\u001b[0m     }\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13695)\u001b[0m   }\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13695)\u001b[0m }\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13695)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13695)\u001b[0m \twhile inferring type of node 'mean_squared_error/cond/output/_21'\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m 2022-10-18 18:19:06.744687: W tensorflow/core/common_runtime/forward_type_inference.cc:231] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected compatible input types, but input 1:\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m type_id: TFT_OPTIONAL\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m args {\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m   type_id: TFT_PRODUCT\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m   args {\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m     type_id: TFT_TENSOR\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m     args {\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m       type_id: TFT_BOOL\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m     }\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m   }\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m }\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m  is neither a subtype nor a supertype of the combined inputs preceding it:\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m type_id: TFT_OPTIONAL\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m args {\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m   type_id: TFT_PRODUCT\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m   args {\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m     type_id: TFT_TENSOR\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m     args {\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m       type_id: TFT_LEGACY_VARIANT\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m     }\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m   }\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m }\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m \twhile inferring type of node 'mean_squared_error/cond/output/_21'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1/Unknown - 3s 3s/step - loss: 103792.5625 - mean_squared_error: 103792.5625\n",
      "      1/Unknown - 3s 3s/step - loss: 103792.5625 - mean_squared_error: 103792.5625\n",
      "      5/Unknown - 3s 13ms/step - loss: nan - mean_squared_error: nan              \n",
      "      5/Unknown - 3s 13ms/step - loss: nan - mean_squared_error: nan              \n",
      "Result for TensorflowTrainer_e22d0_00000:\n",
      "  _time_this_iter_s: 3.653472900390625\n",
      "  _timestamp: 1666084747\n",
      "  _training_iteration: 1\n",
      "  date: 2022-10-18_18-19-08\n",
      "  done: false\n",
      "  experiment_id: 2279cddf94834c3392d3276938acaf67\n",
      "  hostname: YONGJINs-MacBook-Pro.local\n",
      "  iterations_since_restore: 1\n",
      "  loss: .nan\n",
      "  mean_squared_error: .nan\n",
      "  node_ip: 127.0.0.1\n",
      "  pid: 13685\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 9.213613986968994\n",
      "  time_this_iter_s: 9.213613986968994\n",
      "  time_total_s: 9.213613986968994\n",
      "  timestamp: 1666084748\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: e22d0_00000\n",
      "  warmup_time: 0.017626285552978516\n",
      "  \n",
      "8/8 [==============================] - 3s 87ms/step - loss: nan - mean_squared_error: nan\n",
      "8/8 [==============================] - 3s 87ms/step - loss: nan - mean_squared_error: nan\n",
      "      1/Unknown - 0s 58ms/step - loss: nan - mean_squared_error: nan\n",
      "      1/Unknown - 0s 58ms/step - loss: nan - mean_squared_error: nan\n",
      "      8/Unknown - 0s 16ms/step - loss: nan - mean_squared_error: nan\n",
      "      8/Unknown - 0s 16ms/step - loss: nan - mean_squared_error: nan\n",
      "8/8 [==============================] - 0s 33ms/step - loss: nan - mean_squared_error: nan\n",
      "8/8 [==============================] - 0s 33ms/step - loss: nan - mean_squared_error: nan\n",
      "      1/Unknown - 0s 46ms/step - loss: nan - mean_squared_error: nan\n",
      "      1/Unknown - 0s 45ms/step - loss: nan - mean_squared_error: nan\n",
      "      8/Unknown - 0s 15ms/step - loss: nan - mean_squared_error: nan\n",
      "      8/Unknown - 0s 15ms/step - loss: nan - mean_squared_error: nan\n",
      "8/8 [==============================] - 0s 31ms/step - loss: nan - mean_squared_error: nan\n",
      "8/8 [==============================] - 0s 31ms/step - loss: nan - mean_squared_error: nan\n",
      "      4/Unknown - 0s 19ms/step - loss: nan - mean_squared_error: nan\n",
      "      4/Unknown - 0s 19ms/step - loss: nan - mean_squared_error: nan\n",
      "      7/Unknown - 0s 18ms/step - loss: nan - mean_squared_error: nan\n",
      "      7/Unknown - 0s 18ms/step - loss: nan - mean_squared_error: nan\n",
      "8/8 [==============================] - 0s 33ms/step - loss: nan - mean_squared_error: nan\n",
      "8/8 [==============================] - 0s 33ms/step - loss: nan - mean_squared_error: nan\n",
      "Result for TensorflowTrainer_e22d0_00000:\n",
      "  _time_this_iter_s: 0.4578092098236084\n",
      "  _timestamp: 1666084749\n",
      "  _training_iteration: 4\n",
      "  date: 2022-10-18_18-19-09\n",
      "  done: true\n",
      "  experiment_id: 2279cddf94834c3392d3276938acaf67\n",
      "  experiment_tag: '0'\n",
      "  hostname: YONGJINs-MacBook-Pro.local\n",
      "  iterations_since_restore: 4\n",
      "  loss: .nan\n",
      "  mean_squared_error: .nan\n",
      "  node_ip: 127.0.0.1\n",
      "  pid: 13685\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 10.557934999465942\n",
      "  time_this_iter_s: 0.4618649482727051\n",
      "  time_total_s: 10.557934999465942\n",
      "  timestamp: 1666084749\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 4\n",
      "  trial_id: e22d0_00000\n",
      "  warmup_time: 0.017626285552978516\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13695)\u001b[0m /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 4 leaked semaphore objects to clean up at shutdown\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13695)\u001b[0m   warnings.warn('resource_tracker: There appear to be %d '\n",
      "2022-10-18 18:19:09,591\tINFO tune.py:758 -- Total run time: 16.76 seconds (16.63 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': nan, 'mean_squared_error': nan, '_timestamp': 1666084749, '_time_this_iter_s': 0.4578092098236084, '_training_iteration': 4, 'time_this_iter_s': 0.4618649482727051, 'should_checkpoint': True, 'done': True, 'timesteps_total': None, 'episodes_total': None, 'training_iteration': 4, 'trial_id': 'e22d0_00000', 'experiment_id': '2279cddf94834c3392d3276938acaf67', 'date': '2022-10-18_18-19-09', 'timestamp': 1666084749, 'time_total_s': 10.557934999465942, 'pid': 13685, 'hostname': 'YONGJINs-MacBook-Pro.local', 'node_ip': '127.0.0.1', 'config': {}, 'time_since_restore': 10.557934999465942, 'timesteps_since_restore': 0, 'iterations_since_restore': 4, 'warmup_time': 0.017626285552978516, 'experiment_tag': '0'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m Exception ignored in: <function Pool.__del__ at 0x7fad9ce34ee0>\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m   File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/multiprocessing/pool.py\", line 268, in __del__\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m     self._change_notifier.put(None)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m   File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/multiprocessing/queues.py\", line 368, in put\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m     self._writer.send_bytes(obj)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m   File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m     self._send_bytes(m[offset:offset + size])\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m   File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m     self._send(header + buf)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m   File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m     n = write(self._handle, buf)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m OSError: [Errno 9] Bad file descriptor\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m Exception ignored in: <function Pool.__del__ at 0x7fad9ce34ee0>\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m   File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/multiprocessing/pool.py\", line 268, in __del__\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m     self._change_notifier.put(None)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m   File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/multiprocessing/queues.py\", line 368, in put\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m     self._writer.send_bytes(obj)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m   File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m     self._send_bytes(m[offset:offset + size])\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m   File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m     self._send(header + buf)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m   File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m     n = write(self._handle, buf)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=13694)\u001b[0m OSError: [Errno 9] Bad file descriptor\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ray.data.from_items([{\"x\": x, \"y\": 2 * x + 1} for x in range(200)])\n",
    "\n",
    "num_workers = 2\n",
    "use_gpu = False\n",
    "\n",
    "config = {\"lr\": 1e-3, \"batch_size\": 32, \"epochs\": 4}\n",
    "\n",
    "trainer = TensorflowTrainer(\n",
    "    train_loop_per_worker=train_func,\n",
    "    train_loop_config=config,\n",
    "    scaling_config=ScalingConfig(num_workers=num_workers, use_gpu=use_gpu),\n",
    "    datasets={'train': train_dataset},\n",
    ")\n",
    "\n",
    "result = trainer.fit()\n",
    "print(result.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08003162",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
